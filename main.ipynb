{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## E-14 OCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import easyocr\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_PATH = 'test.png'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread(IMAGE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "blur = cv2.GaussianBlur(gray, (5,5), 15)\n",
    "edged = cv2.Canny(blur, 10, 500)\n",
    "plt.imshow(edged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = easyocr.Reader(['es'], gpu=False)\n",
    "result = reader.readtext(edged, allowlist ='0123456789')\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(IMAGE_PATH)\n",
    "for detection in result:\n",
    "    top_left = tuple([int(val) for val in detection[0][0]])\n",
    "    bottom_right = tuple([int(val) for val in detection[0][2]])\n",
    "    text = detection[1]\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    img = cv2.rectangle(img, top_left, bottom_right, (0, 255, 0), 1)\n",
    "    img = cv2.putText(img, text, top_left, font, 0.35, (0, 100, 0), 1, cv2.LINE_AA)\n",
    "    \n",
    "_ = plt.figure(figsize=(20, 15))\n",
    "_ = plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network - Waste Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2 as cv\n",
    "import tensorflow as tf\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR = 'data'\n",
    "train_dataset = tf.keras.preprocessing.image_dataset_from_directory(DIR, validation_split=0.1, subset=\"training\", seed=42, batch_size=128, smart_resize=True, image_size=(256, 256))\n",
    "test_dataset = tf.keras.preprocessing.image_dataset_from_directory(DIR, validation_split=0.1, subset=\"validation\", seed=42, batch_size=128, smart_resize=True, image_size=(256, 256))\n",
    "\n",
    "classes = train_dataset.class_names\n",
    "num_classes = len(classes)\n",
    "print(classes)\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_dataset = train_dataset.prefetch(buffer_size=AUTOTUNE)\n",
    "test_dataset = test_dataset.prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseModel = tf.keras.applications.MobileNetV3Large(input_shape=(256, 256,3), weights='imagenet', include_top=False, classes=num_classes)\n",
    "for layers in baseModel.layers[:-6]:\n",
    "  layers.trainable=False\n",
    "\n",
    "last_output = baseModel.layers[-1].output\n",
    "x = tf.keras.layers.Dropout(0.45) (last_output)\n",
    "x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "x = tf.keras.layers.BatchNormalization() (x)\n",
    "x = tf.keras.layers.Dense(256, activation = tf.keras.activations.elu, kernel_regularizer=tf.keras.regularizers.l1(0.045), activity_regularizer=tf.keras.regularizers.l1(0.045),  kernel_initializer='he_normal')(x)\n",
    "x = tf.keras.layers.Dropout(0.45) (x)\n",
    "x = tf.keras.layers.Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "model = tf.keras.Model(inputs=baseModel.input,outputs=x)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.00125), loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n",
    "\n",
    "epochs = 50\n",
    "lrCallback = tf.keras.callbacks.LearningRateScheduler(lambda epoch: 1e-3 * 10 ** (epoch / 30))\n",
    "stepDecay = tf.keras.callbacks.LearningRateScheduler(lambda epoch: 0.1 * 0.1**math.floor(epoch / 6))\n",
    "history = model.fit(train_dataset, validation_data=test_dataset, epochs=epochs, callbacks=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('model_2.h5')\n",
    "\n",
    "path = 'test_data/trash130.jpg'\n",
    "\n",
    "img = tf.keras.preprocessing.image.load_img(path, target_size=(256, 256))\n",
    "img_array = tf.keras.preprocessing.image.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, 0)\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "\n",
    "plt.imshow(img)\n",
    "print(predictions[0]*100, \"\\n\", classes)\n",
    "print(\"Prediction: \", classes[np.argmax(predictions)], f\"{predictions[0][np.argmax(predictions)]*100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, target_names, cmap=None):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    import itertools\n",
    "\n",
    "    accuracy = np.trace(cm) / float(np.sum(cm))\n",
    "    misclass = 1 - accuracy\n",
    "\n",
    "    if cmap is None:\n",
    "        cmap = plt.get_cmap('Blues')\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title('Confusion matrix')\n",
    "    plt.colorbar()\n",
    "\n",
    "    if target_names is not None:\n",
    "        tick_marks = np.arange(len(target_names))\n",
    "        plt.xticks(tick_marks, target_names, rotation=45)\n",
    "        plt.yticks(tick_marks, target_names)\n",
    "\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label\\naccuracy={:0.4f}%; misclass={:0.4f}%'.format(accuracy, misclass))\n",
    "    plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "true = []\n",
    "predictions = []\n",
    "\n",
    "path = \"data\"\n",
    "for i in os.listdir(path):\n",
    "  folderPath = os.path.join(path, i)\n",
    "  for j in os.listdir(folderPath)[:550]:\n",
    "    fullPath = os.path.join(folderPath, j)\n",
    "    try:\n",
    "      img = tf.keras.preprocessing.image.load_img(fullPath, target_size=(256, 256))\n",
    "      img_array = tf.keras.preprocessing.image.img_to_array(img)\n",
    "      img_array = tf.expand_dims(img_array, 0) \n",
    "\n",
    "      preds = model.predict(img_array)\n",
    "      true.append(classes.index(i))\n",
    "      predictions.append(np.argmax(preds))\n",
    "    except:\n",
    "      print(\"Error on image:\", fullPath)\n",
    "\n",
    "plot_confusion_matrix(tf.math.confusion_matrix(true, predictions), classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carboard: 764<br>\n",
    "Glass: 991<br>\n",
    "Metal: 1031<br>\n",
    "Paper: 594<br>\n",
    "Plastic: 1278"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumExpr defaulting to 8 threads.\n",
      "Migrating database to v0.15.1\n"
     ]
    }
   ],
   "source": [
    "import fiftyone as fo\n",
    "import fiftyone.zoo as foz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading split 'train' to 'C:\\Users\\danie\\fiftyone\\open-images-v6\\train' if necessary\n",
      "Downloading 'https://storage.googleapis.com/openimages/v5/train-annotations-human-imagelabels-boxable.csv' to 'C:\\Users\\danie\\fiftyone\\open-images-v6\\train\\labels\\classifications.csv'\n",
      "Downloading 100 images\n",
      " 100% |███████████████████| 100/100 [10.9s elapsed, 0s remaining, 14.9 files/s]      \n",
      "Dataset info written to 'C:\\Users\\danie\\fiftyone\\open-images-v6\\info.json'\n",
      "Loading existing dataset 'open-images-organic'. To reload from disk, either delete the existing dataset or provide a custom `dataset_name` to use\n"
     ]
    }
   ],
   "source": [
    "dataset = foz.load_zoo_dataset(\n",
    "    \"open-images-v6\", \n",
    "    split=\"train\", \n",
    "    label_types=['classifications']\n",
    "    , \n",
    "    classes=[\"Fruit\",\"Vegetable\"],\n",
    "    max_samples=100,\n",
    "    seed=51,\n",
    "    shuffle=True,\n",
    "    dataset_name=\"open-images-organic\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4d9defa72c2715dab9f7f172572cd30a1ab1a2083462d32ef96aadb7c6e0c73b"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
